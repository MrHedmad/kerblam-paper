@article{netwok2020,
  title={At-scale impact of the {Net Wok}: A culinarically holistic investigation of distributed dumplings},
  author={Astley, Rick and Morris, Linda},
  journal={Armenian Journal of Proceedings},
  volume={61},
  pages={192--219},
  year=2020,
  publisher={Automatic Publishing Inc.}
}

@misc{DataAnalysisCambridge2023,
  title = {Data Analysis in the {{Cambridge Dictionary}}.},
  year = {2023},
  month = dec,
  urldate = {2023-12-20},
  abstract = {the process of examining information, especially using a computer, in order to\ldots},
  howpublished = {https://dictionary.cambridge.org/dictionary/english/data-analysis},
  langid = {english},
  file = {/home/hedmad/Zotero/storage/W7MPYS5J/data-analysis.html}
}
@article{vincentWhoQualifiesBe2015,
  title = {Who Qualifies to Be a Bioinformatician?},
  author = {Vincent, Antony T. and Charette, Steve J.},
  year = {2015},
  month = apr,
  journal = {Frontiers in Genetics},
  volume = {6},
  pages = {164},
  issn = {1664-8021},
  doi = {10.3389/fgene.2015.00164},
  urldate = {2023-12-20},
  pmcid = {PMC4408859},
  pmid = {25964799},
  file = {/home/hedmad/Zotero/storage/5MDEMP7H/Vincent and Charette - 2015 - Who qualifies to be a bioinformatician.pdf}
}
@article{devitoCatalogueBiasPublication2019,
  title = {Catalogue of Bias: Publication Bias},
  shorttitle = {Catalogue of Bias},
  author = {DeVito, Nicholas J. and Goldacre, Ben},
  year = {2019},
  month = apr,
  journal = {BMJ Evidence-Based Medicine},
  volume = {24},
  number = {2},
  pages = {53--54},
  publisher = {{Royal Society of Medicine}},
  issn = {2515-446X, 2515-4478},
  doi = {10.1136/bmjebm-2018-111107},
  urldate = {2023-12-20},
  abstract = {Dickersin and Min define publication bias as the failure to publish the results of a study `on the basis of the direction or strength of the study findings'.1 This non-publication introduces a bias which impacts the ability to accurately synthesise and describe the evidence in a given area.2 Publication bias is a type of reporting bias and closely related to dissemination bias, although dissemination bias generally applies to all forms of results dissemination, not simply journal publications. A variety of distinct biases are often grouped into the overall definition of publication bias.3 4  There are a number of risk factors and causes for publication bias identified in the literature.5 Research has shown causes of publication bias ranging from trialist motivation, past experience, and competing commitments; perceived or real lack of interest in results from editors, reviewers,~or other colleagues; or conflicts of interest that would lead to the suppression of results not aligned with a specific agenda.3 6\textendash 9 The role of journal editors is particularly complex as the gatekeepers to publication. Significant results are more widely cited in medicine aligning the incentives of both investigators and editors towards these studies.10 A review by Song and colleagues reports studies showing that strength and \ldots},
  chapter = {EBM Learning},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2019. No commercial re-use. See rights and permissions. Published by BMJ.},
  langid = {english},
  pmid = {30523135},
  keywords = {medical ethics},
  file = {/home/hedmad/Zotero/storage/I42I65MD/DeVito and Goldacre - 2019 - Catalogue of bias publication bias.pdf}
}
@article{baker500ScientistsLift2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7604},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  urldate = {2023-12-20},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Springer Nature Limited},
  langid = {english},
  keywords = {Peer review,Publishing,Research management},
  file = {/home/hedmad/Zotero/storage/97VXHYQG/Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf;/home/hedmad/Zotero/storage/S3FS8W2T/533452a.html}
}
@misc{molderSustainableDataAnalysis2021a,
  title = {Sustainable Data Analysis with {{Snakemake}}},
  author = {M{\"o}lder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and {Tomkins-Tinch}, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and K{\"o}ster, Johannes},
  year = {2021},
  month = apr,
  number = {10:33},
  institution = {{F1000Research}},
  doi = {10.12688/f1000research.29032.2},
  urldate = {2023-12-20},
  abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way.\&nbsp;Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid. Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {adaptability,data analysis,reproducibility,scalability,sustainability,transparency,workflow management},
  file = {/home/hedmad/Zotero/storage/HDQ74P88/MÃ¶lder et al. - 2021 - Sustainable data analysis with Snakemake.pdf}
}
@article{ditommasoNextflowEnablesReproducible2017,
  title = {Nextflow Enables Reproducible Computational Workflows},
  author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
  year = {2017},
  month = apr,
  journal = {Nature Biotechnology},
  volume = {35},
  number = {4},
  pages = {316--319},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/nbt.3820},
  urldate = {2023-12-20},
  copyright = {2017 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Data publication and archiving},
  file = {/home/hedmad/Zotero/storage/KQVNJKVU/Di Tommaso et al. - 2017 - Nextflow enables reproducible computational workfl.pdf}
}
@book{tukey77,
  title = {Exploratory Data Analysis},
  author = {Tukey, John W.},
  year = {1977},
  publisher = {{Addison-Wesley}},
  added-at = {2009-10-28T04:42:52.000+0100},
  citeulike-article-id = {107137},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  interhash = {69fff218408c5eda560e267ff0099dec},
  intrahash = {e0c6e346a4594eb8f0bc71a6e65366fb},
  keywords = {eda statistics},
  timestamp = {2009-10-28T04:43:20.000+0100}
}
@article{wilkinsonFAIRGuidingPrinciples2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = mar,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160018},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  urldate = {2023-12-21},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders{\textemdash}representing academia, industry, funding agencies, and scholarly publishers{\textemdash}have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  file = {/home/hedmad/Zotero/storage/M2SBBNZS/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf}
}
@misc{GNUBulletinVol,
  title = {{{GNU}}'s {{Bulletin}}, Vol. 1 No. 4 - {{GNU Project}} - {{Free Software Foundation}}},
  urldate = {2023-12-21},
  howpublished = {https://www.gnu.org/bulletins/bull4.html},
  file = {/home/hedmad/Zotero/storage/6JA67DBY/bull4.html}
}
@article{chubinOpenScienceClosed1985,
  title = {Open {{Science}} and {{Closed Science}}: {{Tradeoffs}} in a {{Democracy}}},
  shorttitle = {Open {{Science}} and {{Closed Science}}},
  author = {Chubin, Daryl E.},
  year = {1985},
  month = apr,
  journal = {Science, Technology, \& Human Values},
  volume = {10},
  number = {2},
  pages = {73--80},
  publisher = {{SAGE Publications Inc}},
  issn = {0162-2439},
  doi = {10.1177/016224398501000211},
  urldate = {2023-12-28},
  langid = {english}
}
@article{munafoManifestoReproducibleScience2017a,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  urldate = {2023-12-21},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  copyright = {2017 Macmillan Publishers Limited},
  langid = {english},
  keywords = {Social sciences},
  file = {/home/hedmad/Zotero/storage/2ZLW7SMX/MunafÃ² et al. - 2017 - A manifesto for reproducible science.pdf}
}
@article{simundicBiasResearch2013,
  title = {Bias in Research},
  author = {{\v S}imundi{\'c}, Ana-Maria},
  year = {2013},
  month = feb,
  journal = {Biochemia Medica},
  volume = {23},
  number = {1},
  pages = {12--15},
  issn = {1330-0962},
  doi = {10.11613/BM.2013.003},
  urldate = {2023-12-28},
  abstract = {By writing scientific articles we communicate science among colleagues and peers. By doing this, it is our responsibility to adhere to some basic principles like transparency and accuracy. Authors, journal editors and reviewers need to be concerned about the quality of the work submitted for publication and ensure that only studies which have been designed, conducted and reported in a transparent way, honestly and without any deviation from the truth get to be published. Any such trend or deviation from the truth in data collection, analysis, interpretation and publication is called bias. Bias in research can occur either intentionally or unintentionally. Bias causes false conclusions and is potentially misleading. Therefore, it is immoral and unethical to conduct biased research. Every scientist should thus be aware of all potential sources of bias and undertake all possible actions to reduce or minimize the deviation from the truth. This article describes some basic issues related to bias in research.},
  pmcid = {PMC3900086},
  pmid = {23457761},
  file = {/home/hedmad/Zotero/storage/WX8WJE22/Å imundiÄ - 2013 - Bias in research.pdf}
}
@article{andradeHARKingCherryPickingPHacking2021,
  title = {{{HARKing}}, {{Cherry-Picking}}, {{P-Hacking}}, {{Fishing Expeditions}}, and {{Data Dredging}} and {{Mining}} as {{Questionable Research Practices}}},
  author = {Andrade, Chittaranjan},
  year = {2021},
  month = feb,
  journal = {The Journal of Clinical Psychiatry},
  volume = {82},
  number = {1},
  pages = {25941},
  publisher = {{Physicians Postgraduate Press, Inc.}},
  issn = {0160-6689},
  doi = {10.4088/JCP.20f13804},
  urldate = {2023-12-28},
  abstract = {ABSTRACT Questionable research practices (QRPs) in the statistical analysis of data and in the presentation of the results in research papers include HARKing, cherry-picking, P-hacking, fishing, and data dredging or mining. HARKing (Hypothesizing After the Results are Known) is the presentation of a post hoc hypothesis as an a priori hypothesis. Cherry-picking is the presentation of favorable evidence with the concealment of unfavorable evidence. P-hacking is the relentless analysis of data with an intent to obtain a statistically significant result, usually to support the researcher's hypothesis. A fishing expedition is the indiscriminate testing of associations between different combinations of variables not with specific hypotheses in mind but with the hope of finding something that is statistically significant in the data. Data dredging and data mining describe the extensive testing of relationships between a large number of variables for which data are available, usually in a database. This article explains what these QRPs are and why they are QRPs. This knowledge must become widespread so that researchers and readers understand what approaches to statistical analysis and reporting amount to scientific misconduct.},
  langid = {english},
  file = {/home/hedmad/Zotero/storage/95KKJDQF/Andrade - 2021 - HARKing, Cherry-Picking, P-Hacking, Fishing Expedi.pdf}
}

@article{bertramOpenScience2023,
  title = {Open Science},
  author = {Bertram, Michael G. and Sundin, Josefin and Roche, Dominique G. and {S{\'a}nchez-T{\'o}jar}, Alfredo and Thor{\'e}, Eli S. J. and Brodin, Tomas},
  year = {2023},
  month = aug,
  journal = {Current Biology},
  volume = {33},
  number = {15},
  pages = {R792-R797},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2023.05.036},
  urldate = {2024-02-16},
  abstract = {The term `open science' refers to a range of methods, tools, platforms and practices that aim to make scientific research more accessible, transparent, reproducible and reliable. This includes, for example, sharing code, data and research materials, embracing new publishing formats such as registered reports and preprints, pursuing replication studies and reanalyses, optimising statistical approaches to improve evidence assessment and re-evaluating institutional incentives. The ongoing shift towards open science practices is partly due to mounting evidence that studies across disciplines suffer from biases, underpowered designs and irreproducible or non-replicable results. It also stems from a general desire amongst many researchers to reduce hyper-competitivity in science and instead promote collaborative research that benefits science and society.},
  file = {/home/hedmad/Zotero/storage/XXJ88ICP/Bertram et al. - 2023 - Open science.pdf;/home/hedmad/Zotero/storage/GV49SWQI/S0960982223006681.html}
}

@article{barkerIntroducingFAIRPrinciples2022,
  title = {Introducing the {{FAIR Principles}} for Research Software},
  author = {Barker, Michelle and Chue Hong, Neil P. and Katz, Daniel S. and Lamprecht, Anna-Lena and {Martinez-Ortiz}, Carlos and Psomopoulos, Fotis and Harrow, Jennifer and Castro, Leyla Jael and Gruenpeter, Morane and Martinez, Paula Andrea and Honeyman, Tom},
  year = {2022},
  month = oct,
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  pages = {622},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01710-x},
  urldate = {2024-07-08},
  abstract = {Research software is a fundamental and vital part of research, yet significant challenges to discoverability, productivity, quality, reproducibility, and sustainability exist. Improving the practice of scholarship is a common goal of the open science, open source, and FAIR (Findable, Accessible, Interoperable and Reusable) communities and research software is now being understood as a type of digital object to which FAIR should be applied. This emergence reflects a maturation of the research community to better understand the crucial role of FAIR research software in maximising research value. The FAIR for Research Software (FAIR4RS) Working Group has adapted the FAIR Guiding Principles to create the FAIR Principles for Research Software (FAIR4RS Principles). The contents and context of the FAIR4RS Principles are summarised here to provide the basis for discussion of their adoption. Examples of implementation by organisations are provided to share information on how to maximise the value of research outputs, and to encourage others to amplify the importance and impact of this work.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Policy,Research management},
  file = {/home/hedmad/Zotero/storage/4IST3NTR/Barker et al. - 2022 - Introducing the FAIR Principles for research softw.pdf}
}


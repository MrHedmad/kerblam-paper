@article{netwok2020,
  title={At-scale impact of the {Net Wok}: A culinarically holistic investigation of distributed dumplings},
  author={Astley, Rick and Morris, Linda},
  journal={Armenian Journal of Proceedings},
  volume={61},
  pages={192--219},
  year=2020,
  publisher={Automatic Publishing Inc.}
}
@article{erringtonChallengesAssessingReplicability2021,
  title = {Challenges for Assessing Replicability in Preclinical Cancer Biology},
  author = {Errington, Timothy M and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Rodgers, Peter and Franco, Eduardo},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e67995},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.67995},
  urldate = {2024-08-08},
  abstract = {We conducted the Reproducibility Project: Cancer Biology to investigate the replicability of preclinical research in cancer biology. The initial aim of the project was to repeat 193 experiments from 53 high-impact papers, using an approach in which the experimental protocols and plans for data analysis had to be peer reviewed and accepted for publication before experimental work could begin. However, the various barriers and challenges we encountered while designing and conducting the experiments meant that we were only able to repeat 50 experiments from 23 papers. Here we report these barriers and challenges. First, many original papers failed to report key descriptive and inferential statistics: the data needed to compute effect sizes and conduct power analyses was publicly accessible for just 4 of 193 experiments. Moreover, despite contacting the authors of the original papers, we were unable to obtain these data for 68\% of the experiments. Second, none of the 193 experiments were described in sufficient detail in the original paper to enable us to design protocols to repeat the experiments, so we had to seek clarifications from the original authors. While authors were extremely or very helpful for 41\% of experiments, they were minimally helpful for 9\% of experiments, and not at all helpful (or did not respond to us) for 32\% of experiments. Third, once experimental work started, 67\% of the peer-reviewed protocols required modifications to complete the research and just 41\% of those modifications could be implemented. Cumulatively, these three factors limited the number of experiments that could be repeated. This experience draws attention to a basic and fundamental concern about replication -- it is hard to assess whether reported findings are credible.},
  keywords = {open data,open science,preregistration,replication,reproducibility,Reproducibility Project: Cancer Biology},
  file = {/home/hedmad/Zotero/storage/APHB5JX2/Errington et al. - 2021 - Challenges for assessing replicability in preclini.pdf}
}
@article{ioannidisWhyMostPublished2005a,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  urldate = {2024-08-08},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  file = {/home/hedmad/Zotero/storage/KJLRN97F/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf}
}

@misc{PackagingPythonProjects,
  title = {Packaging {{Python Projects}} - {{Python Packaging User Guide}}},
  urldate = {2024-08-02},
  howpublished = {https://packaging.python.org/en/latest/tutorials/packaging-projects/},
  file = {/home/hedmad/Zotero/storage/FZMGJWJS/packaging-projects.html}
}

@misc{DataAnalysisCambridge2023,
  title = {Data Analysis in the {{Cambridge Dictionary}}.},
  year = {2023},
  month = dec,
  urldate = {2023-12-20},
  abstract = {the process of examining information, especially using a computer, in order to\ldots},
  howpublished = {https://dictionary.cambridge.org/dictionary/english/data-analysis},
  langid = {english},
  file = {/home/hedmad/Zotero/storage/W7MPYS5J/data-analysis.html}
}
@article{vincentWhoQualifiesBe2015,
  title = {Who Qualifies to Be a Bioinformatician?},
  author = {Vincent, Antony T. and Charette, Steve J.},
  year = {2015},
  month = apr,
  journal = {Frontiers in Genetics},
  volume = {6},
  pages = {164},
  issn = {1664-8021},
  doi = {10.3389/fgene.2015.00164},
  urldate = {2023-12-20},
  pmcid = {PMC4408859},
  pmid = {25964799},
  file = {/home/hedmad/Zotero/storage/5MDEMP7H/Vincent and Charette - 2015 - Who qualifies to be a bioinformatician.pdf}
}
@article{devitoCatalogueBiasPublication2019,
  title = {Catalogue of Bias: Publication Bias},
  shorttitle = {Catalogue of Bias},
  author = {DeVito, Nicholas J. and Goldacre, Ben},
  year = {2019},
  month = apr,
  journal = {BMJ Evidence-Based Medicine},
  volume = {24},
  number = {2},
  pages = {53--54},
  publisher = {{Royal Society of Medicine}},
  issn = {2515-446X, 2515-4478},
  doi = {10.1136/bmjebm-2018-111107},
  urldate = {2023-12-20},
  abstract = {Dickersin and Min define publication bias as the failure to publish the results of a study `on the basis of the direction or strength of the study findings'.1 This non-publication introduces a bias which impacts the ability to accurately synthesise and describe the evidence in a given area.2 Publication bias is a type of reporting bias and closely related to dissemination bias, although dissemination bias generally applies to all forms of results dissemination, not simply journal publications. A variety of distinct biases are often grouped into the overall definition of publication bias.3 4  There are a number of risk factors and causes for publication bias identified in the literature.5 Research has shown causes of publication bias ranging from trialist motivation, past experience, and competing commitments; perceived or real lack of interest in results from editors, reviewers,~or other colleagues; or conflicts of interest that would lead to the suppression of results not aligned with a specific agenda.3 6\textendash 9 The role of journal editors is particularly complex as the gatekeepers to publication. Significant results are more widely cited in medicine aligning the incentives of both investigators and editors towards these studies.10 A review by Song and colleagues reports studies showing that strength and \ldots},
  chapter = {EBM Learning},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2019. No commercial re-use. See rights and permissions. Published by BMJ.},
  langid = {english},
  pmid = {30523135},
  keywords = {medical ethics},
  file = {/home/hedmad/Zotero/storage/I42I65MD/DeVito and Goldacre - 2019 - Catalogue of bias publication bias.pdf}
}
@article{baker500ScientistsLift2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7604},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  urldate = {2023-12-20},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Springer Nature Limited},
  langid = {english},
  keywords = {Peer review,Publishing,Research management},
  file = {/home/hedmad/Zotero/storage/97VXHYQG/Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf;/home/hedmad/Zotero/storage/S3FS8W2T/533452a.html}
}
@misc{molderSustainableDataAnalysis2021,
  title = {Sustainable Data Analysis with {{Snakemake}}},
  author = {M{\"o}lder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and {Tomkins-Tinch}, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and K{\"o}ster, Johannes},
  year = {2021},
  month = apr,
  number = {10:33},
  institution = {{F1000Research}},
  doi = {10.12688/f1000research.29032.2},
  urldate = {2023-12-20},
  abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way.\&nbsp;Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid. Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {adaptability,data analysis,reproducibility,scalability,sustainability,transparency,workflow management},
  file = {/home/hedmad/Zotero/storage/HDQ74P88/Mölder et al. - 2021 - Sustainable data analysis with Snakemake.pdf}
}
@article{ditommasoNextflowEnablesReproducible2017,
  title = {Nextflow Enables Reproducible Computational Workflows},
  author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
  year = {2017},
  month = apr,
  journal = {Nature Biotechnology},
  volume = {35},
  number = {4},
  pages = {316--319},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/nbt.3820},
  urldate = {2023-12-20},
  copyright = {2017 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Data publication and archiving},
  file = {/home/hedmad/Zotero/storage/KQVNJKVU/Di Tommaso et al. - 2017 - Nextflow enables reproducible computational workfl.pdf}
}
@book{tukey77,
  title = {Exploratory Data Analysis},
  author = {Tukey, John W.},
  year = {1977},
  publisher = {{Addison-Wesley}},
  added-at = {2009-10-28T04:42:52.000+0100},
  citeulike-article-id = {107137},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  interhash = {69fff218408c5eda560e267ff0099dec},
  intrahash = {e0c6e346a4594eb8f0bc71a6e65366fb},
  keywords = {eda statistics},
  timestamp = {2009-10-28T04:43:20.000+0100}
}
@article{wilkinsonFAIRGuidingPrinciples2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = mar,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160018},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  urldate = {2023-12-21},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders{\textemdash}representing academia, industry, funding agencies, and scholarly publishers{\textemdash}have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  file = {/home/hedmad/Zotero/storage/M2SBBNZS/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf}
}
@misc{GNUBulletinVol,
  title = {{{GNU}}'s {{Bulletin}}, Vol. 1 No. 4 - {{GNU Project}} - {{Free Software Foundation}}},
  urldate = {2023-12-21},
  howpublished = {https://www.gnu.org/bulletins/bull4.html},
  file = {/home/hedmad/Zotero/storage/6JA67DBY/bull4.html}
}
@article{chubinOpenScienceClosed1985,
  title = {Open {{Science}} and {{Closed Science}}: {{Tradeoffs}} in a {{Democracy}}},
  shorttitle = {Open {{Science}} and {{Closed Science}}},
  author = {Chubin, Daryl E.},
  year = {1985},
  month = apr,
  journal = {Science, Technology, \& Human Values},
  volume = {10},
  number = {2},
  pages = {73--80},
  publisher = {{SAGE Publications Inc}},
  issn = {0162-2439},
  doi = {10.1177/016224398501000211},
  urldate = {2023-12-28},
  langid = {english}
}
@article{munafoManifestoReproducibleScience2017a,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  urldate = {2023-12-21},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  copyright = {2017 Macmillan Publishers Limited},
  langid = {english},
  keywords = {Social sciences},
  file = {/home/hedmad/Zotero/storage/2ZLW7SMX/Munafò et al. - 2017 - A manifesto for reproducible science.pdf}
}
@article{simundicBiasResearch2013,
  title = {Bias in Research},
  author = {{\v S}imundi{\'c}, Ana-Maria},
  year = {2013},
  month = feb,
  journal = {Biochemia Medica},
  volume = {23},
  number = {1},
  pages = {12--15},
  issn = {1330-0962},
  doi = {10.11613/BM.2013.003},
  urldate = {2023-12-28},
  abstract = {By writing scientific articles we communicate science among colleagues and peers. By doing this, it is our responsibility to adhere to some basic principles like transparency and accuracy. Authors, journal editors and reviewers need to be concerned about the quality of the work submitted for publication and ensure that only studies which have been designed, conducted and reported in a transparent way, honestly and without any deviation from the truth get to be published. Any such trend or deviation from the truth in data collection, analysis, interpretation and publication is called bias. Bias in research can occur either intentionally or unintentionally. Bias causes false conclusions and is potentially misleading. Therefore, it is immoral and unethical to conduct biased research. Every scientist should thus be aware of all potential sources of bias and undertake all possible actions to reduce or minimize the deviation from the truth. This article describes some basic issues related to bias in research.},
  pmcid = {PMC3900086},
  pmid = {23457761},
  file = {/home/hedmad/Zotero/storage/WX8WJE22/Šimundić - 2013 - Bias in research.pdf}
}
@article{andradeHARKingCherryPickingPHacking2021,
  title = {{{HARKing}}, {{Cherry-Picking}}, {{P-Hacking}}, {{Fishing Expeditions}}, and {{Data Dredging}} and {{Mining}} as {{Questionable Research Practices}}},
  author = {Andrade, Chittaranjan},
  year = {2021},
  month = feb,
  journal = {The Journal of Clinical Psychiatry},
  volume = {82},
  number = {1},
  pages = {25941},
  publisher = {{Physicians Postgraduate Press, Inc.}},
  issn = {0160-6689},
  doi = {10.4088/JCP.20f13804},
  urldate = {2023-12-28},
  abstract = {ABSTRACT Questionable research practices (QRPs) in the statistical analysis of data and in the presentation of the results in research papers include HARKing, cherry-picking, P-hacking, fishing, and data dredging or mining. HARKing (Hypothesizing After the Results are Known) is the presentation of a post hoc hypothesis as an a priori hypothesis. Cherry-picking is the presentation of favorable evidence with the concealment of unfavorable evidence. P-hacking is the relentless analysis of data with an intent to obtain a statistically significant result, usually to support the researcher's hypothesis. A fishing expedition is the indiscriminate testing of associations between different combinations of variables not with specific hypotheses in mind but with the hope of finding something that is statistically significant in the data. Data dredging and data mining describe the extensive testing of relationships between a large number of variables for which data are available, usually in a database. This article explains what these QRPs are and why they are QRPs. This knowledge must become widespread so that researchers and readers understand what approaches to statistical analysis and reporting amount to scientific misconduct.},
  langid = {english},
  file = {/home/hedmad/Zotero/storage/95KKJDQF/Andrade - 2021 - HARKing, Cherry-Picking, P-Hacking, Fishing Expedi.pdf}
}

@article{bertramOpenScience2023,
  title = {Open Science},
  author = {Bertram, Michael G. and Sundin, Josefin and Roche, Dominique G. and {S{\'a}nchez-T{\'o}jar}, Alfredo and Thor{\'e}, Eli S. J. and Brodin, Tomas},
  year = {2023},
  month = aug,
  journal = {Current Biology},
  volume = {33},
  number = {15},
  pages = {R792-R797},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2023.05.036},
  urldate = {2024-02-16},
  abstract = {The term `open science' refers to a range of methods, tools, platforms and practices that aim to make scientific research more accessible, transparent, reproducible and reliable. This includes, for example, sharing code, data and research materials, embracing new publishing formats such as registered reports and preprints, pursuing replication studies and reanalyses, optimising statistical approaches to improve evidence assessment and re-evaluating institutional incentives. The ongoing shift towards open science practices is partly due to mounting evidence that studies across disciplines suffer from biases, underpowered designs and irreproducible or non-replicable results. It also stems from a general desire amongst many researchers to reduce hyper-competitivity in science and instead promote collaborative research that benefits science and society.},
  file = {/home/hedmad/Zotero/storage/XXJ88ICP/Bertram et al. - 2023 - Open science.pdf;/home/hedmad/Zotero/storage/GV49SWQI/S0960982223006681.html}
}

@article{barkerIntroducingFAIRPrinciples2022,
  title = {Introducing the {{FAIR Principles}} for Research Software},
  author = {Barker, Michelle and Chue Hong, Neil P. and Katz, Daniel S. and Lamprecht, Anna-Lena and {Martinez-Ortiz}, Carlos and Psomopoulos, Fotis and Harrow, Jennifer and Castro, Leyla Jael and Gruenpeter, Morane and Martinez, Paula Andrea and Honeyman, Tom},
  year = {2022},
  month = oct,
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  pages = {622},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01710-x},
  urldate = {2024-07-08},
  abstract = {Research software is a fundamental and vital part of research, yet significant challenges to discoverability, productivity, quality, reproducibility, and sustainability exist. Improving the practice of scholarship is a common goal of the open science, open source, and FAIR (Findable, Accessible, Interoperable and Reusable) communities and research software is now being understood as a type of digital object to which FAIR should be applied. This emergence reflects a maturation of the research community to better understand the crucial role of FAIR research software in maximising research value. The FAIR for Research Software (FAIR4RS) Working Group has adapted the FAIR Guiding Principles to create the FAIR Principles for Research Software (FAIR4RS Principles). The contents and context of the FAIR4RS Principles are summarised here to provide the basis for discussion of their adoption. Examples of implementation by organisations are provided to share information on how to maximise the value of research outputs, and to encourage others to amplify the importance and impact of this work.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Policy,Research management},
  file = {/home/hedmad/Zotero/storage/4IST3NTR/Barker et al. - 2022 - Introducing the FAIR Principles for research softw.pdf}
}

@misc{communityTuringWayHandbook2022,
  title = {The {{Turing Way}}: {{A}} Handbook for Reproducible, Ethical and Collaborative Research},
  shorttitle = {The {{Turing Way}}},
  author = {Community, The Turing Way},
  year = {2022},
  month = jul,
  doi = {10.5281/zenodo.7625728},
  urldate = {2024-07-12},
  abstract = {The Turing Way: A handbook for reproducible, ethical and collaborative research The Turing Way December 2022 Latest The Turing Way is an open source community-driven guide to reproducible, ethical, inclusive and collaborative data science. The Turing Way book is collaboratively developed by its diverse community of researchers, learners, educators, and other stakeholders. The Turing Way project is openly developed and any and all questions, comments and recommendations are welcome at our github repository: https://github.com/alan-turing-institute/the-turing-way. In 2020, the project underwent a major overhaul categorising chapters into 5 guides on reproducible research, project design, collaboration, communication and ethical research. Additionally, we added a community handbook to document all the practices designed and implemented towards the development of the project and community. This release in 2021 includes additional chapters developed by our contributors across five guides and the community handbook. In addition, all the project documents from the project are provided as they appear on The Turing Way GitHub repository including the Zenodo metadata: https://github.com/alan-turing-institute/the-turing-way. Release log v1.1.0: Zenodo metadata information and additional chapters from Book Dash Dec 2022 v1.0.3: Zenodo metadata information and additional chapters from Book Dash May 2022 v1.0.2: Zenodo metadata information and additional chapters since Book Dash November 2021 v1.0.1: Zenodo metadata information and additional chapters. v1.0.0: Five guide expansion of The Turing Way with a community handbook v0.0.4: Continuous integration chapter merged to main. v0.0.3: Reproducible environments chapter merged to main. v0.0.2: Version control chapter merged to main. v0.0.1: Reproducibility chapter merged to main. Full Changelog: https://github.com/alan-turing-institute/the-turing-way/compare/v1.0.1...v1.0.3 (Previous release: https://github.com/alan-turing-institute/the-turing-way/compare/v0.0.3...v1.0.1) v1.1.0},
  howpublished = {Zenodo},
  keywords = {collaboration,community,data science,ethics,handbook,reproducibility,research practices},
  file = {/home/hedmad/Zotero/storage/YICQ7LWP/7625728.html}
}

@misc{PackageStructureState,
  title = {3~ {{Package}} Structure and State -- {{R Packages}} (2e)},
  urldate = {2024-08-02},
  howpublished = {https://r-pkgs.org/structure.html},
  file = {/home/hedmad/Zotero/storage/KNTJ3FQ9/structure.html}
}

@misc{CreatingNewPackage,
  title = {Creating a {{New Package}} - {{The Cargo Book}}},
  urldate = {2024-08-02},
  howpublished = {https://doc.rust-lang.org/cargo/guide/creating-a-new-project.html},
  file = {/home/hedmad/Zotero/storage/KA8YQ9VI/creating-a-new-project.html}
}


@misc{GitHubFlow,
  title = {{{GitHub}} Flow},
  journal = {GitHub Docs},
  urldate = {2024-08-08},
  abstract = {Follow GitHub flow to collaborate on projects.},
  howpublished = {https://docs.github.com/en/get-started/using-github/github-flow},
  langid = {english},
  file = {/home/hedmad/Zotero/storage/ASZI8GGG/github-flow.html}
}

@inproceedings{appletonStreamedLinesBranching1998,
  title = {Streamed {{Lines}}: {{Branching Patterns}} for {{Parallel Software Development}}},
  shorttitle = {Streamed {{Lines}}},
  author = {Appleton, Brad and Berczuk, S. and Cabrera, R.},
  year = {1998},
  urldate = {2024-08-08},
  abstract = {Most software version control systems provide mechanisms for branching into multiple lines of development and merging source code from one development line into another. However, the techniques, policies and guidelines for using these mechanisms are often misapplied or not fully understood. This is unfortunate, since the use or misuse of branching and merging can make or break a parallel software development project. Streamed Lines is a pattern language for organizing related lines of development into appropriately diverging and converging streams of source code changes.},
  file = {/home/hedmad/Zotero/storage/KKLIYUV9/Appleton et al. - 1998 - Streamed Lines Branching Patterns for Parallel So.pdf}
}

@misc{SphinxSphinxDocumentation,
  title = {Sphinx --- {{Sphinx}} Documentation},
  urldate = {2024-08-08},
  howpublished = {https://www.sphinx-doc.org/en/master/},
  file = {/home/hedmad/Zotero/storage/NPD6DAJI/master.html}
}

@article{crusoeMethodsIncludedStandardizing2022,
  title = {Methods Included: Standardizing Computational Reuse and Portability with the {{Common Workflow Language}}},
  shorttitle = {Methods Included},
  author = {Crusoe, Michael R. and Abeln, Sanne and Iosup, Alexandru and Amstutz, Peter and Chilton, John and Tijani{\'c}, Neboj{\v s}a and M{\'e}nager, Herv{\'e} and {Soiland-Reyes}, Stian and Gavrilovi{\'c}, Bogdan and Goble, Carole and Community, The CWL},
  year = {2022},
  month = may,
  journal = {Commun. ACM},
  volume = {65},
  number = {6},
  pages = {54--63},
  issn = {0001-0782},
  doi = {10.1145/3486897},
  urldate = {2024-08-08},
  abstract = {Standardizing computational reuse and portability with the Common Workflow Language.},
  file = {/home/hedmad/Zotero/storage/FHZ2X3NJ/Crusoe et al. - 2022 - Methods included standardizing computational reus.pdf}
}
